{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Basic libraries\n",
    "#\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import numpy    as np\n",
    "import pandas   as pd\n",
    "from   datetime import datetime\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Sklearn\n",
    "#\n",
    "from sklearn                 import metrics\n",
    "from sklearn                 import preprocessing\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# XGBoost\n",
    "#\n",
    "import xgboost\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# User libraries\n",
    "#\n",
    "from utils.Logger   import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other parameters\n",
    "#\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate logger\n",
    "#\n",
    "if VERBOSE:\n",
    "    logger = init_logger( log_file = 'logs.log' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "- Irrigation \n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "The scope is to predict if a region is 'irrigated' or 'drainaged' based on satellite multi-temporal data (indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/Irrigation_train.csv')\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info(f'Training data were loaded')\n",
    "    logger.info(f'Number of instances:  {df_train.shape[0]}')\n",
    "    logger.info(f'Number of features:   {df_train.shape[1]}')\n",
    "\n",
    "df_train.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('Data/Irrigation_test.csv')\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info(f'Testing data were loaded')\n",
    "    logger.info(f'Number of instances:  {df_test.shape[0]}')\n",
    "    logger.info(f'Number of features:   {df_test.shape[1]}')\n",
    "\n",
    "df_test.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Label-Encoder\n",
    "#\n",
    "LabelEncoding = preprocessing.LabelEncoder()\n",
    "\n",
    "# Fit encoder\n",
    "#\n",
    "LabelEncoding.fit( df_train[ 'Irrigation' ] )\n",
    "\n",
    "# Apply encoder\n",
    "df_train[ 'Irrigation' ] = LabelEncoding.transform( df_train['Irrigation' ] )\n",
    "df_test[ 'Irrigation' ]  = LabelEncoding.transform( df_test[ 'Irrigation']  )\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info('Target class was transformed using Label-Encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Validation data\n",
    "trainX = df_train.iloc[:, :-1]\n",
    "trainY = df_train.iloc[:,  -1]\n",
    "\n",
    "# Testing data\n",
    "testX  = df_test.iloc[:, :-1]\n",
    "testY  = df_test.iloc[:,  -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate MLFlow server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate mlflow server\n",
    "# Command: mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 --port 5000\n",
    "# \n",
    "import mlflow\n",
    "from   mlflow.models.signature import infer_signature\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://0.0.0.0:5000/\")\n",
    "mlflow.set_experiment(\"Irrigation-Experiment\")\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info('MLFlow server is connected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provide Model's Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Load model using Model-ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Model-ID\n",
    "ModelID = '24a8d9e18b4d4b28b39799e55b160013'\n",
    "#\n",
    "logged_model = 'runs:/{}/models'.format( ModelID )\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info('Model loaded using Model-ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "#\n",
    "pred = loaded_model.predict( trainX )\n",
    "\n",
    "# Calculate Confusion Matrix (CM)\n",
    "#\n",
    "CM  = metrics.confusion_matrix(trainY, pred)\n",
    "#\n",
    "#\n",
    "logger.info( 30*\"-\" )\n",
    "logger.info( \"*** Training set - Evaluation ***\")\n",
    "logger.info( \"> Accuracy:  %.2f%%\" % (100.0*metrics.accuracy_score( pred, trainY )) )\n",
    "logger.info( \"> AUC:       %.3f\"   % metrics.roc_auc_score(pred, trainY) )\n",
    "logger.info( \"> Recall:    %.3f\"   % metrics.recall_score(trainY, pred) )\n",
    "logger.info( \"> Precision: %.3f\"   % metrics.precision_score(trainY, pred) )\n",
    "logger.info( \"> GM:        %.3f\\n\" % (math.sqrt( np.diag( CM ).prod() ) / math.sqrt( CM[0, :].sum() * CM[1, :].sum() )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "#\n",
    "pred = loaded_model.predict( testX )\n",
    "\n",
    "# Calculate Confusion Matrix (CM)\n",
    "#\n",
    "CM  = metrics.confusion_matrix(testY, pred)\n",
    "#\n",
    "#\n",
    "logger.info( 30*\"-\" )\n",
    "logger.info( \"*** Testing set - Evaluation ***\")\n",
    "logger.info( \"> Accuracy:  %.2f%%\" % (100.0*metrics.accuracy_score( pred, testY )) )\n",
    "logger.info( \"> AUC:       %.3f\"   % metrics.roc_auc_score(pred, testY) )\n",
    "logger.info( \"> Recall:    %.3f\"   % metrics.recall_score(testY, pred) )\n",
    "logger.info( \"> Precision: %.3f\"   % metrics.precision_score(testY, pred) )\n",
    "logger.info( \"> GM:        %.3f\\n\" % (math.sqrt( np.diag( CM ).prod() ) / math.sqrt( CM[0, :].sum() * CM[1, :].sum() )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Load registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching an MLflow Model from the Model Registry\n",
    "# Notice that the model status should be 'Staging'\n",
    "#\n",
    "import mlflow.pyfunc\n",
    "\n",
    "model_name = \"Irrigation_model\"\n",
    "model_version = 1\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(\n",
    "    model_uri=f\"models:/{model_name}/{model_version}\"\n",
    ")\n",
    "\n",
    "\n",
    "# > Fetch the latest model version in a specific stage\n",
    "# > To fetch a model version by stage, simply provide the model stage as part of the model URI, and it will fetch the most recent version of the model in that stage.\n",
    "#\n",
    "# model_name = \"Irrigation_model\"\n",
    "# stage = 'Production'\n",
    "\n",
    "# loaded_model = mlflow.pyfunc.load_model(\n",
    "#     model_uri=f\"models:/{model_name}/{stage}\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "#\n",
    "pred = loaded_model.predict( trainX )\n",
    "\n",
    "# Calculate Confusion Matrix (CM)\n",
    "#\n",
    "CM  = metrics.confusion_matrix(trainY, pred)\n",
    "#\n",
    "#\n",
    "logger.info( 30*\"-\" )\n",
    "logger.info( \"*** Training set - Evaluation ***\")\n",
    "logger.info( \"> Accuracy:  %.2f%%\" % (100.0*metrics.accuracy_score( pred, trainY )) )\n",
    "logger.info( \"> AUC:       %.3f\"   % metrics.roc_auc_score(pred, trainY) )\n",
    "logger.info( \"> Recall:    %.3f\"   % metrics.recall_score(trainY, pred) )\n",
    "logger.info( \"> Precision: %.3f\"   % metrics.precision_score(trainY, pred) )\n",
    "logger.info( \"> GM:        %.3f\\n\" % (math.sqrt( np.diag( CM ).prod() ) / math.sqrt( CM[0, :].sum() * CM[1, :].sum() )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "#\n",
    "pred = loaded_model.predict( testX )\n",
    "\n",
    "# Calculate Confusion Matrix (CM)\n",
    "#\n",
    "CM  = metrics.confusion_matrix(testY, pred)\n",
    "#\n",
    "#\n",
    "logger.info( 30*\"-\" )\n",
    "logger.info( \"*** Testing set - Evaluation ***\")\n",
    "logger.info( \"> Accuracy:  %.2f%%\" % (100.0*metrics.accuracy_score( pred, testY )) )\n",
    "logger.info( \"> AUC:       %.3f\"   % metrics.roc_auc_score(pred, testY) )\n",
    "logger.info( \"> Recall:    %.3f\"   % metrics.recall_score(testY, pred) )\n",
    "logger.info( \"> Precision: %.3f\"   % metrics.precision_score(testY, pred) )\n",
    "logger.info( \"> GM:        %.3f\\n\" % (math.sqrt( np.diag( CM ).prod() ) / math.sqrt( CM[0, :].sum() * CM[1, :].sum() )) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "afea5bdb149cee4bb6ddbc8de92100e6e2504969dafe00b7c6b2f8f5c0fb2a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
