{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "#\n",
    "import math\n",
    "import json\n",
    "import numpy    as np\n",
    "import pandas   as pd\n",
    "from   datetime import datetime\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "#\n",
    "from sklearn                 import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn                 import preprocessing\n",
    "\n",
    "\n",
    "# Sklearn-Optimization\n",
    "#\n",
    "import skopt\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "#\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO parameters \n",
    "n_calls         = 10\n",
    "n_random_starts = 2\n",
    "\n",
    "# XGBoost - parameters\n",
    "n_estimators          = 5\n",
    "early_stopping_rounds = 10\n",
    "seed                  = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "- Irrigation \n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "The score is to predict if a region is 'irrigated' or 'drainaged' based on satellite multi-temporal data (indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/Irrigation_train.csv')\n",
    "\n",
    "print('[INFO] Number of instances: ', df_train.shape[0])\n",
    "print('[INFO] Number of features:  ', df_train.shape[1])\n",
    "\n",
    "df_train.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('Data/Irrigation_test.csv')\n",
    "\n",
    "print('[INFO] Number of instances: ', df_test.shape[0])\n",
    "print('[INFO] Number of features:  ', df_test.shape[1])\n",
    "\n",
    "df_test.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Label-Encoder\n",
    "#\n",
    "LabelEncoding = preprocessing.LabelEncoder()\n",
    "\n",
    "# Fit encoder\n",
    "#\n",
    "LabelEncoding.fit( df_train[ 'Irrigation' ] )\n",
    "\n",
    "# Apply encoder\n",
    "df_train[ 'Irrigation' ] = LabelEncoding.transform( df_train['Irrigation' ] )\n",
    "df_test[ 'Irrigation' ]  = LabelEncoding.transform( df_test[ 'Irrigation']  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Validation data\n",
    "trainX = df_train.iloc[:, :-1]\n",
    "trainY = df_train.iloc[:,  -1]\n",
    "trainX, validX, trainY, validY = train_test_split(trainX, trainY, test_size = 0.2, random_state=42)\n",
    "\n",
    "# # Convert dataset to special XGBoost optimised data structure\n",
    "# dtrain = xgboost.DMatrix(trainX, label = trainY)\n",
    "# dvalid = xgboost.DMatrix(validX, label = validY)\n",
    "\n",
    "\n",
    "# Testing data\n",
    "testX  = df_test.iloc[:, :-1]\n",
    "testY  = df_test.iloc[:,  -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from   mlflow.models.signature import infer_signature\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://0.0.0.0:5000/\")\n",
    "mlflow.set_experiment(\"Irrigation-Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter_Evaluation():\n",
    "    def __init__(self, trainX, trainY, validX, validY):\n",
    "        # Data\n",
    "        self.trainX = trainX\n",
    "        self.trainY = trainY\n",
    "        self.validX = validX\n",
    "        self.validY = validY\n",
    "        # Number of iterations\n",
    "        self.Iter        = 1\n",
    "        # Best score\n",
    "        self.best_score  = 0\n",
    "        \n",
    "    def select_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "        \n",
    "    def evaluate_params(self, params):\n",
    "        \n",
    "        tag     = {\"Simulation\" : \"sample-\" + str(self.Iter), \"model\": \"XGBoost\"}\n",
    "        runname = \"XGBoost-test-run-\" + str(self.Iter)\n",
    "\n",
    "        with mlflow.start_run(run_name = runname) as run:\n",
    "            # Tags to help in tracking\n",
    "            mlflow.set_tags(tag)\n",
    "\n",
    "            # Log params/hyperparameters used in experiement\n",
    "            mlflow.log_params(params)\n",
    "            \n",
    "\n",
    "            # Setup model\n",
    "            #\n",
    "            model =  self.model.set_params( **params )\n",
    "            \n",
    "            # Train model\n",
    "            #\n",
    "            model.fit(self.trainX, self.trainY, \n",
    "                    eval_metric = 'auc', \n",
    "                    eval_set = [ (self.validX, self.validY) ],\n",
    "                    early_stopping_rounds = early_stopping_rounds);\n",
    "\n",
    "        \n",
    "            # Evaluation\n",
    "            #\n",
    "            pred = model.predict( self.validX )\n",
    "            #\n",
    "            Accuracy  = 100.0 * metrics.accuracy_score( pred, self.validY )\n",
    "            try:\n",
    "                AUC   = metrics.roc_auc_score( pred, self.validY )\n",
    "            except:\n",
    "                AUC   = 0.0\n",
    "            Recall    = metrics.recall_score( pred, self.validY )\n",
    "            Precision = metrics.precision_score( pred, self.validY )        \n",
    "            \n",
    "            # Calculate Confusion Matrix (CM)\n",
    "            CM = metrics.confusion_matrix(self.validY, pred)\n",
    "            GM = math.sqrt( np.diag( CM ).prod() ) / math.sqrt( CM[0, :].sum() * CM[1, :].sum() )\n",
    "\n",
    "            \n",
    "            # Export results\n",
    "            if (AUC > self.best_score):\n",
    "                print(\"Iteration {:3.0f} with Accuracy = {:6.3f} AUC = {:6.3f} at {} (best) \".format(self.Iter, Accuracy, AUC, str(datetime.now().time())[:8]))\n",
    "                self.best_score = AUC\n",
    "            else:\n",
    "                print(\"Iteration {:3.0f} with Accuracy = {:6.3f} AUC = {:6.3f} at {} \".format(self.Iter, Accuracy, AUC, str(datetime.now().time())[:8]))\n",
    "            \n",
    "\n",
    "            mlflow.log_metric(\"Accuracy\",       Accuracy)\n",
    "            mlflow.log_metric(\"AUC\",            AUC)\n",
    "            mlflow.log_metric(\"Recall\",         Recall)\n",
    "            mlflow.log_metric(\"Precision\",      Precision)\n",
    "            mlflow.log_metric(\"Geometric Mean\", GM)\n",
    "            \n",
    "            signature = infer_signature(self.validX, pred)\n",
    "            \n",
    "            # Log model created\n",
    "            mlflow.sklearn.log_model(model, artifact_path = \"models\", signature = signature) \n",
    "\n",
    "        mlflow.end_run()\n",
    "          \n",
    "        # Update Iteration counter\n",
    "        self.Iter += 1\n",
    "        \n",
    "        \n",
    "        return( -AUC )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Parameter_Evaluation(trainX, trainY, validX, validY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "#\n",
    "model = xgboost.XGBClassifier(n_estimators        = n_estimators, \n",
    "                              n_jobs              = -1, \n",
    "                              objective           = 'binary:logistic', \n",
    "                              validate_parameters = True, \n",
    "                              verbosity           = 1) \n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost\n",
    "\n",
    "\n",
    "\n",
    "# # Convert dataset to special XGBoost optimised data structure\n",
    "# dtrain_matrix = xgboost.DMatrix(trainX, label = trainY)\n",
    "# # dcomp_matrix  = xgb.DMatrix(cd_enc)\n",
    "\n",
    "\n",
    "# # List of parameters\n",
    "# params = {\n",
    "#     'booster': 'gbtree',\n",
    "#     'objective': 'reg:squarederror',\n",
    "#     'learning_rate': 0.3,\n",
    "#     'n_jobs': -1,\n",
    "# }\n",
    "\n",
    "# # Fit the model\n",
    "# xgb_cv = xgboost.cv(params                = params,\n",
    "#                     dtrain                = dtrain_matrix,\n",
    "#                     num_boost_round       = num_boost_round,\n",
    "#                     nfold                 = nfold, \n",
    "#                     show_stdv             = False,\n",
    "#                     metrics               = ['auc', 'aucpr'], \n",
    "#                     as_pandas             = True,\n",
    "#                     stratified            = True,\n",
    "#                     seed                  = seed,\n",
    "#                     early_stopping_rounds = early_stopping_rounds, \n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "search_space = [ \n",
    "                 skopt.space.Real(0.01, 0.1,  name='learning_rate'),\n",
    "                 skopt.space.Integer(5, 10,   name='max_depth'),\n",
    "                 #\n",
    "                 skopt.space.Real(1, 9,       name='gamma'),\n",
    "                 skopt.space.Integer(40, 180, name='reg_alpha'),\n",
    "                 skopt.space.Real(0, 1,       name='reg_lambda'),\n",
    "                 #\n",
    "                 skopt.space.Integer(1, 10,   name='min_child_weight'),\n",
    "                 skopt.space.Real(0.5, 1,     name='reg_lambda'),\n",
    "                 #\n",
    "                 skopt.space.Categorical(categories = ['gbtree', 'dart'], name = \"booster\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPO_params = {\n",
    "              'n_calls':         n_calls,\n",
    "              'n_random_starts': n_random_starts,\n",
    "              'base_estimator':  'ET',\n",
    "              'acq_func':        'EI',\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.select_model( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@skopt.utils.use_named_args( search_space )\n",
    "def objective( **params ):\n",
    "    return  evaluator.evaluate_params( params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = skopt.forest_minimize(objective, search_space, **HPO_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_named_params(results, search_space):\n",
    "    params       = results.x\n",
    "    param_dict   = {}\n",
    "    \n",
    "    params_list  =[(dimension.name, param) for dimension, param in zip(search_space, params)]\n",
    "    \n",
    "    for item in params_list:\n",
    "        param_dict[item[0]] = item[1]\n",
    "    \n",
    "    return( param_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = to_named_params(results, search_space)\n",
    "\n",
    "\n",
    "print('[INFO] Optimized hyperparameters\\n')\n",
    "for (parameter,value) in best_params.items():\n",
    "    if ( isinstance(value, float) ):\n",
    "        print(' >%25s: %.3f' % (parameter,value))\n",
    "    else:\n",
    "        print(' >%25s: %s' % (parameter,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized (best) model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "#\n",
    "model.set_params( **best_params )\n",
    "\n",
    "# Train model\n",
    "#\n",
    "model.fit(trainX, trainY,\n",
    "          eval_metric = 'auc',           \n",
    "          eval_set = [ (validX, validY) ],\n",
    "          early_stopping_rounds = 10);\n",
    "\n",
    "print('[INFO] Model trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "#\n",
    "pred = model.predict( testX )\n",
    "\n",
    "# Calculate Confusion Matrix (CM)\n",
    "#\n",
    "CM  = metrics.confusion_matrix(testY, pred)\n",
    "#\n",
    "#\n",
    "print( \"> Accuracy:  %.2f\" % metrics.accuracy_score( pred, testY ) )\n",
    "print( \"> AUC:       %.3f\" % metrics.roc_auc_score(pred, testY) )\n",
    "print( \"> Recall:    %.3f\" % metrics.recall_score(testY, pred) )\n",
    "print( \"> Precision: %.3f\" % metrics.precision_score(testY, pred) )\n",
    "print( \"> GM:        %.3f\" % (math.sqrt( np.diag( CM ).prod() ) / math.sqrt( CM[0, :].sum() * CM[1, :].sum() )) )\n",
    "\n",
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "afea5bdb149cee4bb6ddbc8de92100e6e2504969dafe00b7c6b2f8f5c0fb2a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
