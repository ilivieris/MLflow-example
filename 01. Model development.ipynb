{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Basic libraries\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import numpy    as np\n",
    "import pandas   as pd\n",
    "from   datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Sklearn\n",
    "from sklearn                 import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn                 import preprocessing\n",
    "from sklearn.metrics         import ConfusionMatrixDisplay\n",
    "from sklearn.metrics         import PrecisionRecallDisplay\n",
    "import skopt\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# XGBoost\n",
    "import xgboost\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# User libraries\n",
    "from utils.Logger   import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO parameters \n",
    "n_calls         =  10\n",
    "n_random_starts =  1\n",
    "\n",
    "# XGBoost - parameters\n",
    "n_estimators          = 1000\n",
    "early_stopping_rounds = 50\n",
    "seed                  = 42\n",
    "\n",
    "# Other parameters\n",
    "VERBOSE = True\n",
    "\n",
    "# Create directory for storing output figures\n",
    "if not os.path.isdir('Figures'): os.mkdir('Figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate logger\n",
    "#\n",
    "if VERBOSE:\n",
    "    logger = init_logger( log_file = 'logs.log' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/train_data.csv')\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info(f'Training data were loaded')\n",
    "    logger.info(f'Number of instances:  {df_train.shape[0]}')\n",
    "    logger.info(f'Number of features:   {df_train.shape[1]}')\n",
    "\n",
    "df_train.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('Data/test_data.csv')\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info(f'Testing data were loaded')\n",
    "    logger.info(f'Number of instances:  {df_test.shape[0]}')\n",
    "    logger.info(f'Number of features:   {df_test.shape[1]}')\n",
    "\n",
    "df_test.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Validation data\n",
    "trainX = df_train.iloc[:, :-1]\n",
    "trainY = df_train.iloc[:,  -1]\n",
    "trainX, validX, trainY, validY = train_test_split(trainX, trainY, test_size = 0.2, random_state = seed)\n",
    "\n",
    "# # Convert dataset to special XGBoost optimised data structure\n",
    "# dtrain = xgboost.DMatrix(trainX, label = trainY)\n",
    "# dvalid = xgboost.DMatrix(validX, label = validY)\n",
    "\n",
    "\n",
    "# Testing data\n",
    "testX  = df_test.iloc[:, :-1]\n",
    "testY  = df_test.iloc[:,  -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate mlflow server\n",
    "# Command: mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 127.0.0.1 --port 5000\n",
    "# \n",
    "import mlflow\n",
    "from   mlflow.models.signature import infer_signature\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:1983/\")\n",
    "mlflow.set_experiment(\"NVCR-Experiments\")\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info('MLFlow server is connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter_Evaluation():\n",
    "    def __init__(self, trainX, trainY, validX, validY, VERBOSE = True):\n",
    "        # Data\n",
    "        self.trainX = trainX\n",
    "        self.trainY = trainY\n",
    "        self.validX = validX\n",
    "        self.validY = validY\n",
    "        # \n",
    "        self.VERBOSE = VERBOSE\n",
    "        # Number of iterations\n",
    "        self.Iter = 1\n",
    "        # Best score\n",
    "        self.best_score = 0\n",
    "        \n",
    "    def select_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def get_performance_evaluation(self, y, pred):\n",
    "        Accuracy  = 100.0 * metrics.accuracy_score(y, pred)\n",
    "        try:\n",
    "            AUC   = metrics.roc_auc_score(y, pred)\n",
    "        except:\n",
    "            AUC   = 0.0\n",
    "        Recall    = metrics.recall_score(y, pred)\n",
    "        Precision = metrics.precision_score(y, pred)     \n",
    "\n",
    "        # Calculate Confusion Matrix (CM)\n",
    "        CM = metrics.confusion_matrix(y, pred)\n",
    "        GM = math.sqrt( np.diag( CM ).prod() ) / math.sqrt( CM[0, :].sum() * CM[1, :].sum() )\n",
    "            \n",
    "        return Accuracy, AUC, Precision, Recall, GM, CM\n",
    "    \n",
    "    def evaluate_params(self, params):\n",
    "        \n",
    "        tag     = {\"Simulation\" : \"sample-\" + str(self.Iter), \"model\": \"XGBoost\"}\n",
    "        runname = \"XGBoost-test-run-\" + str(self.Iter)\n",
    "\n",
    "        with mlflow.start_run(run_name = runname) as run:\n",
    "            # Tags to help in tracking\n",
    "            mlflow.set_tags(tag)\n",
    "            \n",
    "            # Log params/hyperparameters used in experiement\n",
    "            mlflow.log_params(params)\n",
    "            \n",
    "    \n",
    "            # Setup model\n",
    "            #\n",
    "            model =  self.model.set_params( **params )\n",
    "            \n",
    "            # Train model\n",
    "            #\n",
    "            model.fit(self.trainX, self.trainY, \n",
    "                      eval_metric = 'auc', \n",
    "                      eval_set = [ (self.trainX, self.trainY), (self.validX, self.validY) ],\n",
    "                      early_stopping_rounds = early_stopping_rounds,\n",
    "                      verbose = 100);\n",
    "\n",
    "            \n",
    "            # Evaluation on Training set\n",
    "            pred = model.predict( self.trainX )        \n",
    "            Accuracy, AUC, Precision, Recall, GM, CM = self.get_performance_evaluation(self.trainY.values, pred)\n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metric(\"train_Accuracy\", Accuracy)\n",
    "            mlflow.log_metric(\"train_AUC\", AUC)\n",
    "            mlflow.log_metric(\"train_Recall\", Recall)\n",
    "            mlflow.log_metric(\"train_Precision\", Precision)\n",
    "            mlflow.log_metric(\"train_GM\", GM)\n",
    "            \n",
    "            # Confusion matrix (Training)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=CM, display_labels=model.classes_)\n",
    "            ax = plt.figure(figsize=(4,4)).gca()\n",
    "            fig = disp.plot(ax=ax,cmap = 'Blues', colorbar=False);\n",
    "            plt.title('Confusion Matrix - Train')\n",
    "            fig.figure_.savefig('Figures/CM_train.png', dpi=100)\n",
    "            plt.show()\n",
    "            \n",
    "            # PR-curve (Training)\n",
    "            ax = plt.figure(figsize=(4,3)).gca()\n",
    "            display = PrecisionRecallDisplay.from_estimator(model, self.trainX, self.trainY, ax=ax)\n",
    "            display.ax_.set_title(\"2-class Precision-Recall curve\");\n",
    "            display.figure_.savefig('Figures/PR-curve_train.png', dpi=100)\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            # Evaluation on Validation set\n",
    "            pred = model.predict( self.validX )        \n",
    "            Accuracy, AUC, Precision, Recall, GM, CM = self.get_performance_evaluation(self.validY.values, pred)\n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metric(\"valid_Accuracy\", Accuracy)\n",
    "            mlflow.log_metric(\"valid_AUC\", AUC)\n",
    "            mlflow.log_metric(\"valid_Recall\", Recall)\n",
    "            mlflow.log_metric(\"valid_Precision\", Precision)\n",
    "            mlflow.log_metric(\"valid_GM\", GM)\n",
    "            \n",
    "            # Confusion matrix (Testing)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=CM, display_labels=model.classes_)\n",
    "            ax = plt.figure(figsize=(4, 4)).gca()\n",
    "            fig = disp.plot(ax=ax,cmap = 'Blues', colorbar=False);\n",
    "            plt.title('Confusion Matrix - Test')\n",
    "            fig.figure_.savefig('Figures/CM_test.png', dpi=100)\n",
    "            plt.show()\n",
    "            \n",
    "            # PR-curve (Testing)\n",
    "            ax = plt.figure(figsize=(4,3)).gca()\n",
    "            display = PrecisionRecallDisplay.from_estimator(model, self.validX, self.validY, ax=ax)\n",
    "            display.ax_.set_title(\"2-class Precision-Recall curve\");\n",
    "            display.figure_.savefig('Figures/PR-curve_test.png', dpi=100)\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Export results\n",
    "            if (AUC > self.best_score): self.best_score = AUC\n",
    "            if self.VERBOSE:\n",
    "                logger.info( \"Iteration {:3.0f} with Accuracy = {:6.3f}% AUC = {:6.3f} GM = {:6.3f}\".format(self.Iter, Accuracy, AUC, GM) )\n",
    "            \n",
    "            \n",
    "            # Include model signature\n",
    "            signature = infer_signature(self.validX, pred)\n",
    "            \n",
    "            # Log model created\n",
    "            mlflow.sklearn.log_model(model, artifact_path = \"models\", signature = signature) \n",
    "\n",
    "            mlflow.log_artifacts(\"Data\", artifact_path=\"Data\")\n",
    "            mlflow.log_artifacts(\"Figures\", artifact_path=\"Figures\")\n",
    "            \n",
    "        mlflow.end_run()\n",
    "          \n",
    "        # Update Iteration counter\n",
    "        self.Iter += 1\n",
    "        \n",
    "        \n",
    "        return( -AUC )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Parameter_Evaluation(trainX, trainY, validX, validY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "#\n",
    "model = xgboost.XGBClassifier(n_estimators        = n_estimators, \n",
    "                              n_jobs              = -1, \n",
    "                              objective           = 'binary:logistic', \n",
    "                              validate_parameters = True, \n",
    "                              verbosity           = 1,\n",
    "                              tree_method         = 'hist',\n",
    "                              gamma               = 1.5,\n",
    "                              reg_alpha           = 20,\n",
    "                              reg_lambda          = 0.7) \n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info('Model was setup')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "search_space = [ \n",
    "                 skopt.space.Categorical([0.1, 0.05, 0.01, 0.05], name='learning_rate'),\n",
    "                 skopt.space.Integer(3, 15,   name='max_depth'),\n",
    "                 #\n",
    "                 skopt.space.Integer(2, 10,   name='min_child_weight'),\n",
    "                 skopt.space.Integer(2, 5,    name='max_leaves'),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPO_params = {\n",
    "              'n_calls':         n_calls,\n",
    "              'n_random_starts': n_random_starts,\n",
    "              'base_estimator':  'ET',\n",
    "              'acq_func':        'EI',\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.select_model( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@skopt.utils.use_named_args( search_space )\n",
    "def objective( **params ):\n",
    "    return  evaluator.evaluate_params( params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = skopt.forest_minimize(objective, search_space, **HPO_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_named_params(results, search_space):\n",
    "    params       = results.x\n",
    "    param_dict   = {}\n",
    "    \n",
    "    params_list  =[(dimension.name, param) for dimension, param in zip(search_space, params)]\n",
    "    \n",
    "    for item in params_list:\n",
    "        param_dict[item[0]] = item[1]\n",
    "    \n",
    "    return( param_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = to_named_params(results, search_space)\n",
    "\n",
    "\n",
    "print('[INFO] Optimized hyperparameters\\n')\n",
    "for (parameter,value) in best_params.items():\n",
    "    if ( isinstance(value, float) ):\n",
    "        print(' >%25s: %.3f' % (parameter,value))\n",
    "    else:\n",
    "        print(' >%25s: %s' % (parameter,value))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Store optimized hyperparameters\n",
    "#\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "\n",
    "with open('checkpoint/Hyperparameters.json', 'w', encoding='utf-8') as f:\n",
    "    f.write( json.dumps( best_params, default = np_encoder ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
