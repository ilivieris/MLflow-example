{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Basic libraries\n",
    "#\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import numpy    as np\n",
    "import pandas   as pd\n",
    "from   datetime import datetime\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Sklearn\n",
    "#\n",
    "from sklearn                 import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn                 import preprocessing\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Sklearn-Optimization\n",
    "#\n",
    "import skopt\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# XGBoost\n",
    "#\n",
    "import xgboost\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# User libraries\n",
    "#\n",
    "from utils.Logger   import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO parameters \n",
    "#\n",
    "n_calls         = 100\n",
    "n_random_starts =  10\n",
    "\n",
    "\n",
    "# XGBoost - parameters\n",
    "#\n",
    "n_estimators          = 1000\n",
    "early_stopping_rounds = 50\n",
    "seed                  = 42\n",
    "\n",
    "\n",
    "# Other parameters\n",
    "#\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate logger\n",
    "#\n",
    "if VERBOSE:\n",
    "    logger = init_logger( log_file = 'logs.log' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "- Irrigation \n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "The scope is to predict if a region is 'irrigated' or 'drainaged' based on satellite multi-temporal data (indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/Irrigation_train.csv')\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info(f'Training data were loaded')\n",
    "    logger.info(f'Number of instances:  {df_train.shape[0]}')\n",
    "    logger.info(f'Number of features:   {df_train.shape[1]}')\n",
    "\n",
    "df_train.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('Data/Irrigation_test.csv')\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info(f'Testing data were loaded')\n",
    "    logger.info(f'Number of instances:  {df_test.shape[0]}')\n",
    "    logger.info(f'Number of features:   {df_test.shape[1]}')\n",
    "\n",
    "df_test.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Label-Encoder\n",
    "#\n",
    "LabelEncoding = preprocessing.LabelEncoder()\n",
    "\n",
    "# Fit encoder\n",
    "#\n",
    "LabelEncoding.fit( df_train[ 'Irrigation' ] )\n",
    "\n",
    "# Apply encoder\n",
    "df_train[ 'Irrigation' ] = LabelEncoding.transform( df_train['Irrigation' ] )\n",
    "df_test[ 'Irrigation' ]  = LabelEncoding.transform( df_test[ 'Irrigation']  )\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info('Target class was transformed using Label-Encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Validation data\n",
    "trainX = df_train.iloc[:, :-1]\n",
    "trainY = df_train.iloc[:,  -1]\n",
    "trainX, validX, trainY, validY = train_test_split(trainX, trainY, test_size = 0.1, random_state = seed)\n",
    "\n",
    "# # Convert dataset to special XGBoost optimised data structure\n",
    "# dtrain = xgboost.DMatrix(trainX, label = trainY)\n",
    "# dvalid = xgboost.DMatrix(validX, label = validY)\n",
    "\n",
    "\n",
    "# Testing data\n",
    "testX  = df_test.iloc[:, :-1]\n",
    "testY  = df_test.iloc[:,  -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate mlflow server\n",
    "# Command: mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 --port 5000\n",
    "# \n",
    "import mlflow\n",
    "from   mlflow.models.signature import infer_signature\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://0.0.0.0:5000/\")\n",
    "mlflow.set_experiment(\"Irrigation-Experiment\")\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info('MLFlow server is connected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter_Evaluation():\n",
    "    def __init__(self, trainX, trainY, validX, validY, VERBOSE = True):\n",
    "        # Data\n",
    "        self.trainX = trainX\n",
    "        self.trainY = trainY\n",
    "        self.validX = validX\n",
    "        self.validY = validY\n",
    "        # \n",
    "        self.VERBOSE = VERBOSE\n",
    "        # Number of iterations\n",
    "        self.Iter        = 1\n",
    "        # Best score\n",
    "        self.best_score  = 0\n",
    "        \n",
    "    def select_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "        \n",
    "    def evaluate_params(self, params):\n",
    "        \n",
    "        tag     = {\"Simulation\" : \"sample-\" + str(self.Iter), \"model\": \"XGBoost\"}\n",
    "        runname = \"XGBoost-test-run-\" + str(self.Iter)\n",
    "\n",
    "        with mlflow.start_run(run_name = runname) as run:\n",
    "            # Tags to help in tracking\n",
    "            mlflow.set_tags(tag)\n",
    "\n",
    "            # Log params/hyperparameters used in experiement\n",
    "            mlflow.log_params(params)\n",
    "            \n",
    "\n",
    "            # Setup model\n",
    "            #\n",
    "            model =  self.model.set_params( **params )\n",
    "            \n",
    "            # Train model\n",
    "            #\n",
    "            model.fit(self.trainX, self.trainY, \n",
    "                    eval_metric = 'auc', \n",
    "                    eval_set = [ (self.validX, self.validY) ],\n",
    "                    early_stopping_rounds = early_stopping_rounds);\n",
    "\n",
    "        \n",
    "            # Evaluation\n",
    "            #\n",
    "            pred = model.predict( self.validX )\n",
    "            #\n",
    "            Accuracy  = 100.0 * metrics.accuracy_score( pred, self.validY )\n",
    "            try:\n",
    "                AUC   = metrics.roc_auc_score( pred, self.validY )\n",
    "            except:\n",
    "                AUC   = 0.0\n",
    "            Recall    = metrics.recall_score( pred, self.validY )\n",
    "            Precision = metrics.precision_score( pred, self.validY )        \n",
    "            \n",
    "            # Calculate Confusion Matrix (CM)\n",
    "            CM = metrics.confusion_matrix(self.validY, pred)\n",
    "            GM = math.sqrt( np.diag( CM ).prod() ) / math.sqrt( CM[0, :].sum() * CM[1, :].sum() )\n",
    "\n",
    "            \n",
    "            # Export results\n",
    "            if (AUC > self.best_score): self.best_score = AUC\n",
    "            \n",
    "            if self.VERBOSE:\n",
    "                logger.info( \"Iteration {:3.0f} with Accuracy = {:6.3f}% AUC = {:6.3f} GM = {:6.3f}\".format(self.Iter, Accuracy, AUC, GM) )\n",
    "               \n",
    "            \n",
    "\n",
    "            mlflow.log_metric(\"Accuracy\",       Accuracy)\n",
    "            mlflow.log_metric(\"AUC\",            AUC)\n",
    "            mlflow.log_metric(\"Recall\",         Recall)\n",
    "            mlflow.log_metric(\"Precision\",      Precision)\n",
    "            mlflow.log_metric(\"Geometric Mean\", GM)\n",
    "            \n",
    "            signature = infer_signature(self.validX, pred)\n",
    "            \n",
    "            # Log model created\n",
    "            mlflow.sklearn.log_model(model, artifact_path = \"models\", signature = signature) \n",
    "\n",
    "        mlflow.end_run()\n",
    "          \n",
    "        # Update Iteration counter\n",
    "        self.Iter += 1\n",
    "        \n",
    "        \n",
    "        return( -AUC )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Parameter_Evaluation(trainX, trainY, validX, validY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "#\n",
    "model = xgboost.XGBClassifier(n_estimators        = n_estimators, \n",
    "                              n_jobs              = -1, \n",
    "                              objective           = 'binary:logistic', \n",
    "                              validate_parameters = True, \n",
    "                              verbosity           = 1) \n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info('Model was setup')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost\n",
    "\n",
    "\n",
    "\n",
    "# # Convert dataset to special XGBoost optimised data structure\n",
    "# dtrain_matrix = xgboost.DMatrix(trainX, label = trainY)\n",
    "# # dcomp_matrix  = xgb.DMatrix(cd_enc)\n",
    "\n",
    "\n",
    "# # List of parameters\n",
    "# params = {\n",
    "#     'booster': 'gbtree',\n",
    "#     'objective': 'reg:squarederror',\n",
    "#     'learning_rate': 0.3,\n",
    "#     'n_jobs': -1,\n",
    "# }\n",
    "\n",
    "# # Fit the model\n",
    "# xgb_cv = xgboost.cv(params                = params,\n",
    "#                     dtrain                = dtrain_matrix,\n",
    "#                     num_boost_round       = num_boost_round,\n",
    "#                     nfold                 = nfold, \n",
    "#                     show_stdv             = False,\n",
    "#                     metrics               = ['auc', 'aucpr'], \n",
    "#                     as_pandas             = True,\n",
    "#                     stratified            = True,\n",
    "#                     seed                  = seed,\n",
    "#                     early_stopping_rounds = early_stopping_rounds, \n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "search_space = [ \n",
    "                 skopt.space.Real(0.01, 0.1,  name='learning_rate'),\n",
    "                 skopt.space.Integer(3, 15,   name='max_depth'),\n",
    "                 #\n",
    "                 skopt.space.Real(1, 9,       name='gamma'),\n",
    "                 skopt.space.Integer(40, 180, name='reg_alpha'),\n",
    "                 skopt.space.Real(0, 1,       name='reg_lambda'),\n",
    "                 #\n",
    "                 skopt.space.Integer(2, 10,   name='min_child_weight'),\n",
    "                 skopt.space.Integer(2, 5,    name='max_leaves'),\n",
    "                 #\n",
    "                 skopt.space.Categorical(categories = ['gbtree', 'dart'], name = \"booster\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPO_params = {\n",
    "              'n_calls':         n_calls,\n",
    "              'n_random_starts': n_random_starts,\n",
    "              'base_estimator':  'ET',\n",
    "              'acq_func':        'EI',\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.select_model( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@skopt.utils.use_named_args( search_space )\n",
    "def objective( **params ):\n",
    "    return  evaluator.evaluate_params( params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = skopt.forest_minimize(objective, search_space, **HPO_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_named_params(results, search_space):\n",
    "    params       = results.x\n",
    "    param_dict   = {}\n",
    "    \n",
    "    params_list  =[(dimension.name, param) for dimension, param in zip(search_space, params)]\n",
    "    \n",
    "    for item in params_list:\n",
    "        param_dict[item[0]] = item[1]\n",
    "    \n",
    "    return( param_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = to_named_params(results, search_space)\n",
    "\n",
    "\n",
    "print('[INFO] Optimized hyperparameters\\n')\n",
    "for (parameter,value) in best_params.items():\n",
    "    if ( isinstance(value, float) ):\n",
    "        print(' >%25s: %.3f' % (parameter,value))\n",
    "    else:\n",
    "        print(' >%25s: %s' % (parameter,value))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Store optimized hyperparameters\n",
    "#\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "\n",
    "with open('checkpoint/Hyperparameters.json', 'w', encoding='utf-8') as f:\n",
    "    f.write( json.dumps( best_params, default = np_encoder ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized (best) model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "#\n",
    "model.set_params( **best_params )\n",
    "logger.info('Optimized-Model was loaded')\n",
    "\n",
    "# Train model\n",
    "#\n",
    "model.fit(trainX, trainY,\n",
    "          eval_metric = 'auc',           \n",
    "          eval_set = [ (validX, validY) ],\n",
    "          early_stopping_rounds = 10);\n",
    "\n",
    "if VERBOSE:\n",
    "    logger.info('Optimized-Model trained')\n",
    "\n",
    "\n",
    "# Save trained model\n",
    "#\n",
    "import pickle\n",
    "filename = 'checkpoint/model.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "if VERBOSE:\n",
    "    logger.info(f'Model saved in {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "#\n",
    "pred = model.predict( testX )\n",
    "\n",
    "# Calculate Confusion Matrix (CM)\n",
    "#\n",
    "CM  = metrics.confusion_matrix(testY, pred)\n",
    "#\n",
    "#\n",
    "logger.info( 30*\"-\" )\n",
    "logger.info( \"*** Evaluation ***\")\n",
    "logger.info( \"> Accuracy:  %.2f%%\" % (100*metrics.accuracy_score( pred, testY )) )\n",
    "logger.info( \"> AUC:       %.3f\"   % metrics.roc_auc_score(pred, testY) )\n",
    "logger.info( \"> Recall:    %.3f\"   % metrics.recall_score(testY, pred) )\n",
    "logger.info( \"> Precision: %.3f\"   % metrics.precision_score(testY, pred) )\n",
    "logger.info( \"> GM:        %.3f\\n\" % (math.sqrt( np.diag( CM ).prod() ) / math.sqrt( CM[0, :].sum() * CM[1, :].sum() )) )\n",
    "\n",
    "\n",
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelID = '0a7e804a25b74a18b560817b8d871e48'\n",
    "# logged_model = 'runs:/{}/models'.format( ModelID )\n",
    "# loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# # Get predictions\n",
    "# #\n",
    "# pred = loaded_model.predict( testX )\n",
    "\n",
    "# # Calculate Confusion Matrix (CM)\n",
    "# #\n",
    "# CM  = metrics.confusion_matrix(testY, pred)\n",
    "# #\n",
    "# #\n",
    "# logger.info( 30*\"-\" )\n",
    "# logger.info( \"*** Evaluation ***\")\n",
    "# logger.info( \"> Accuracy:  %.2f%%\" % metrics.accuracy_score( pred, testY ) )\n",
    "# logger.info( \"> AUC:       %.3f\"   % metrics.roc_auc_score(pred, testY) )\n",
    "# logger.info( \"> Recall:    %.3f\"   % metrics.recall_score(testY, pred) )\n",
    "# logger.info( \"> Precision: %.3f\"   % metrics.precision_score(testY, pred) )\n",
    "# logger.info( \"> GM:        %.3f\\n\" % (math.sqrt( np.diag( CM ).prod() ) / math.sqrt( CM[0, :].sum() * CM[1, :].sum() )) )\n",
    "\n",
    "\n",
    "# CM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "afea5bdb149cee4bb6ddbc8de92100e6e2504969dafe00b7c6b2f8f5c0fb2a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
